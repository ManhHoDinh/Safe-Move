{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b93c76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working/\n",
    "!rm -r climategan/\n",
    "! git clone https://github.com/ManhHoDinh/climategan.git\n",
    "%cd climategan\n",
    "!pip install -r requirements-3.8.2.txt # or `requirements-any.txt` for other Python versions (not tested but expected to be fine)\n",
    "\n",
    "\n",
    "!pip install gdown\n",
    "!mkdir config\n",
    "!mkdir output\n",
    "%cd config\n",
    "!gdown https://drive.google.com/u/0/uc?id=18OCUIy7JQ2Ow_-cC5xn_hhDn-Bp45N1K\n",
    "!unzip release-github-v1.zip\n",
    "%cd /kaggle/working/\n",
    "\n",
    "# Install detectron2 (colab has CUDA 10.1 + torch 1.5)\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)\n",
    "\n",
    "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "!pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision\n",
    "!mkdir -p {HOME}/weights\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights\n",
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
    "\n",
    "%cd ..\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
    "mask_predictor = SamPredictor(sam)\n",
    "\n",
    "# helper function that loads an image before adding it to the widget\n",
    "\n",
    "import base64\n",
    "\n",
    "def encode_image(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
    "    return \"data:image/jpg;base64,\"+encoded\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Union, Optional\n",
    "from dataclasses_json import dataclass_json\n",
    "from supervision import Detections\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOCategory:\n",
    "    id: int\n",
    "    name: str\n",
    "    supercategory: str\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOImage:\n",
    "    id: int\n",
    "    width: int\n",
    "    height: int\n",
    "    file_name: str\n",
    "    license: int\n",
    "    date_captured: str\n",
    "    coco_url: Optional[str] = None\n",
    "    flickr_url: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOAnnotation:\n",
    "    id: int\n",
    "    image_id: int\n",
    "    category_id: int\n",
    "    segmentation: List[List[float]]\n",
    "    area: float\n",
    "    bbox: Tuple[float, float, float, float]\n",
    "    iscrowd: int\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOLicense:\n",
    "    id: int\n",
    "    name: str\n",
    "    url: str\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOJson:\n",
    "    images: List[COCOImage]\n",
    "    annotations: List[COCOAnnotation]\n",
    "    categories: List[COCOCategory]\n",
    "    licenses: List[COCOLicense]\n",
    "\n",
    "\n",
    "def load_coco_json(json_file: str) -> COCOJson:\n",
    "    import json\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    return COCOJson.from_dict(json_data)\n",
    "\n",
    "\n",
    "class COCOJsonUtility:\n",
    "    @staticmethod\n",
    "    def get_annotations_by_image_id(coco_data: COCOJson, image_id: int) -> List[COCOAnnotation]:\n",
    "        return [annotation for annotation in coco_data.annotations if annotation.image_id == image_id]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_annotations_by_image_path(coco_data: COCOJson, image_path: str) -> Optional[List[COCOAnnotation]]:\n",
    "        image = COCOJsonUtility.get_image_by_path(coco_data, image_path)\n",
    "        if image:\n",
    "            return COCOJsonUtility.get_annotations_by_image_id(coco_data, image.id)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_image_by_path(coco_data: COCOJson, image_path: str) -> Optional[COCOImage]:\n",
    "        for image in coco_data.images:\n",
    "            if image.file_name == image_path:\n",
    "                return image\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def annotations2detections(annotations: List[COCOAnnotation]) -> Detections:\n",
    "        class_id, xyxy = [], []\n",
    "\n",
    "        for annotation in annotations:\n",
    "            x_min, y_min, width, height = annotation.bbox\n",
    "            class_id.append(annotation.category_id)\n",
    "            xyxy.append([\n",
    "                x_min,\n",
    "                y_min,\n",
    "                x_min + width,\n",
    "                y_min + height\n",
    "            ])\n",
    "\n",
    "        return Detections(\n",
    "            xyxy=np.array(xyxy, dtype=int),\n",
    "            class_id=np.array(class_id, dtype=int)\n",
    "        )\n",
    "!pip install roboflow\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize_and_overlay_images(foreground, background):\n",
    "    # Resize foreground to match the size of the background\n",
    "    foreground_resized = cv2.resize(foreground, (background.shape[1], background.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Separate the channels of the foreground image\n",
    "    foreground_resized_rgb = foreground_resized[:, :, :3]  # RGB channels\n",
    "    alpha_channel = foreground_resized[:, :, 3]  # Alpha channel\n",
    "\n",
    "    # Normalize the alpha mask to keep intensity between 0 and 1\n",
    "    alpha_mask = alpha_channel.astype(float) / 255\n",
    "\n",
    "    # Make alpha_mask 3 channels\n",
    "    alpha_mask_3d = np.stack([alpha_mask, alpha_mask, alpha_mask], axis=-1)\n",
    "\n",
    "    # Multiply the foreground with the alpha matte\n",
    "    foreground_resized_rgb = cv2.multiply(alpha_mask_3d, foreground_resized_rgb.astype(float))\n",
    "\n",
    "    # Multiply the background with ( 1 - alpha )\n",
    "    background_rgb = background[:, :, :3]  # Get the RGB (assuming the fourth channel is alpha)\n",
    "    alpha_mask_inv = 1 - alpha_mask_3d\n",
    "    background_rgb = cv2.multiply(alpha_mask_inv, background_rgb.astype(float))\n",
    "\n",
    "    # Add the foreground and background.\n",
    "    combined_rgb = cv2.add(foreground_resized_rgb, background_rgb)\n",
    "\n",
    "    # Return the combined image\n",
    "    combined_image = combined_rgb.astype(np.uint8)\n",
    "\n",
    "    return combined_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def insertImageToImage(foreground, originImage):\n",
    "    foreground=resize_and_overlay_images(foreground, originImage)\n",
    "    # Convert the OpenCV images to PIL format, assuming the OpenCV images are in BGR format.\n",
    "    foreground_pil = Image.fromarray(cv2.cvtColor(foreground, cv2.COLOR_BGRA2RGBA))\n",
    "    background_pil = Image.fromarray(cv2.cvtColor(originImage, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Since the images are the same size, we can paste the foreground on the background directly.\n",
    "    background_pil.paste(foreground_pil, (0, 0), foreground_pil)\n",
    "\n",
    "    # Convert back to OpenCV format\n",
    "    result = cv2.cvtColor(np.array(background_pil), cv2.COLOR_RGB2BGR)\n",
    "    return result\n",
    "!pip install roboflow"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
